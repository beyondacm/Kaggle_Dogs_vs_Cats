{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will present a few simple yet effective methods that you can use to build a powerful image classifier, using only very few training examples - just a few hundred pictures from each class you want to be able to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will go over the following options:\n",
    "1. training a small network from scratch(as a baseline)\n",
    "2. using the bottleneck features of a pre-trained network.\n",
    "3. fine-tuning the top layers of a pre-trained newtwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will lead us to cover the following Keras features:\n",
    "1. fit_generator for training Keras a model using Python data generators\n",
    "2. ImageDataGenerator for real-time data augmentation\n",
    "3. layer freezing and model fine-tuning\n",
    "4. ...and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our setup: only 2000 training examples(1000 per class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our examples we will use two sets of pictures, which we get from Kaggle:\n",
    "1000 cats and 1000 dogs(although the original dataset had 12,500 cats and 12,500 dogs, we just took the first 1000 images for each class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is very few examples to learn from, for a classification problem that is far from simple.\n",
    "So this is a challenging machine learning problem, but it is also a realistic one:in a lot of real-world use cases, even small-scale data collection can be extremely expensive or sometimes near-impossible(e.g. in medical imaging).Being able to make the most out of very little data is a key skill of competent data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting competition, top entrants were able to score over 98% accuracy by using modern deep learning techniques. In our case, because we restrict ourselves to only 8% of the dataset, the problem is much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the relevance of deep learning for small-data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A message that I hear often is that \"deep learning is only relevant when you have a huge amount of data.\"\n",
    "While not entirely incorrect, this is somewhat misleading.\n",
    "Certainly, deep learning requires the ability to learn features automatically from the data, which is generally only possible when lots of training data is available -- especially for problems where the input samples are very high-dimensional, like images.  \n",
    "However, convolutional neural networks -- a pillar algorithm of deep learning -- are by design one of the best models available for the most \"perceptual\" problems(such as image classification), even with very little data to learn from.  \n",
    "Training a connnet from scratch on a small image dataset will still yield reasonable results, without the need for any custom feature engineering. Convnets are just plain good. They are the right tool for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But whats more, deep learning models are by nature highly repurposable:  \n",
    "you can take, say, an image classification or speech-to-text model trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes, as we will see in this post. Specifically in the case of computer vision, many pre-trained models(usually trained on the ImageNet dataset) are now publicly available for download and can be used to bootstrap powerful vision models out of very little data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class. This class allows you to:\n",
    "1. configure random transformations and normalization operations to be done on your image data during training\n",
    "2. instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs, fit_generator, evaluate_generator and predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode=\"nearest\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = load_img(\"./Dogs_vs_Cats/train/cat.0.jpg\") # This is a PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (374, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "x = img_to_array(img) # This is a Numpy array with shape(3, 150, 150)\n",
    "print(type(x), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x.reshape((1,) + x.shape) # This is a Numpy array with shape(1, 3, 150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the \"preview/\" directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1, save_to_dir=\"./Dogs_vs_Cats/preview\", save_prefix=\"cat\", save_format=\"jpeg\"):\n",
    "    i = i + 1\n",
    "    if i > 20:\n",
    "        break # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a small convnet from scratch: 80% accuracy in 40 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right tool for an image classification job is a convnet, so let us try to train one on our data, as an initial baseline. Since we only have few examples, our number one concern should be overfitting. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is one way to fight overfitting, but is is not enough since our augmented samples are still highly correlated. Your main focus for fighting overfitting should be the entropic capacity of your model -- how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in your model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to take smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we will use a very small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation(you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the code below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by\n",
    "# max-pooling layers. This is very similar to the architectures that Yann LeCun advocated in the 1990s for image \n",
    "# classification.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of your images\n",
    "img_width, img_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"./data/train/\"\n",
    "validation_data_dir = \"./data/validation/\"\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prepare our data. We will use .flow_from_directory() to generate batches of image data(and their labels)\n",
    "directly from our jpgs in their respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True\n",
    "                )\n",
    "\n",
    "\n",
    "# This is the augmentation configuration we will use for testing\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is a generator that will read pictures found in subfolders of \"data/train\", \n",
    "# and indefinitely generate batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    'data/train/', # this is the target directory\n",
    "                    target_size = (150,150), # all images will be resized to 150x150\n",
    "                    batch_size = batch_size,\n",
    "                    class_mode = \"binary\" # since we use binary_crossentropy loss, we need binary labels\n",
    "                    )\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                        \"data/validation/\",\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = batch_size,\n",
    "                        class_mode = \"binary\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 13s - loss: 0.7145 - acc: 0.5010 - val_loss: 0.6895 - val_acc: 0.5012\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 10s - loss: 0.6950 - acc: 0.5640 - val_loss: 0.6538 - val_acc: 0.7125\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 11s - loss: 0.6511 - acc: 0.6295 - val_loss: 0.6673 - val_acc: 0.5814\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 11s - loss: 0.6299 - acc: 0.6540 - val_loss: 0.5896 - val_acc: 0.7010\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 11s - loss: 0.5959 - acc: 0.6910 - val_loss: 0.5678 - val_acc: 0.6870\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 11s - loss: 0.5796 - acc: 0.6980 - val_loss: 0.7375 - val_acc: 0.6285\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 11s - loss: 0.5618 - acc: 0.7200 - val_loss: 0.5688 - val_acc: 0.6781\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 11s - loss: 0.5494 - acc: 0.7380 - val_loss: 0.5214 - val_acc: 0.7303\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 11s - loss: 0.5292 - acc: 0.7430 - val_loss: 1.4904 - val_acc: 0.5929\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 10s - loss: 0.5330 - acc: 0.7585 - val_loss: 0.5228 - val_acc: 0.7303\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 11s - loss: 0.5080 - acc: 0.7590 - val_loss: 0.5154 - val_acc: 0.7557\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 11s - loss: 0.4859 - acc: 0.7670 - val_loss: 0.5128 - val_acc: 0.7379\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 11s - loss: 0.4929 - acc: 0.7775 - val_loss: 0.5048 - val_acc: 0.7519\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 11s - loss: 0.4946 - acc: 0.7825 - val_loss: 0.5122 - val_acc: 0.7252\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 11s - loss: 0.4753 - acc: 0.7815 - val_loss: 0.4878 - val_acc: 0.7545\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 10s - loss: 0.4841 - acc: 0.7780 - val_loss: 0.5168 - val_acc: 0.7608\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 11s - loss: 0.4799 - acc: 0.7810 - val_loss: 0.5409 - val_acc: 0.7557\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 10s - loss: 0.4580 - acc: 0.7925 - val_loss: 0.4816 - val_acc: 0.7812\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 11s - loss: 0.4479 - acc: 0.7985 - val_loss: 0.4890 - val_acc: 0.7824\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 10s - loss: 0.4463 - acc: 0.8020 - val_loss: 0.5214 - val_acc: 0.7595\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 11s - loss: 0.4434 - acc: 0.8095 - val_loss: 0.5729 - val_acc: 0.7328\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 11s - loss: 0.4535 - acc: 0.8000 - val_loss: 0.4777 - val_acc: 0.7672\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 11s - loss: 0.4387 - acc: 0.8055 - val_loss: 0.4740 - val_acc: 0.7850\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 11s - loss: 0.4247 - acc: 0.8125 - val_loss: 0.5668 - val_acc: 0.7354\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 10s - loss: 0.4256 - acc: 0.8060 - val_loss: 0.4928 - val_acc: 0.8041\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 11s - loss: 0.4119 - acc: 0.8160 - val_loss: 0.5334 - val_acc: 0.7595\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 10s - loss: 0.4259 - acc: 0.8205 - val_loss: 0.4451 - val_acc: 0.7990\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 10s - loss: 0.4156 - acc: 0.8245 - val_loss: 0.6449 - val_acc: 0.7188\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 11s - loss: 0.4145 - acc: 0.8165 - val_loss: 0.4671 - val_acc: 0.7990\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 11s - loss: 0.4001 - acc: 0.8200 - val_loss: 0.5271 - val_acc: 0.7786\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 11s - loss: 0.4177 - acc: 0.8210 - val_loss: 0.5009 - val_acc: 0.8092\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 10s - loss: 0.4169 - acc: 0.8160 - val_loss: 0.4811 - val_acc: 0.7697\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 10s - loss: 0.4171 - acc: 0.8180 - val_loss: 0.5208 - val_acc: 0.8117\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 10s - loss: 0.4104 - acc: 0.8255 - val_loss: 0.5913 - val_acc: 0.7774\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 10s - loss: 0.4157 - acc: 0.8280 - val_loss: 0.5005 - val_acc: 0.7735\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 11s - loss: 0.4010 - acc: 0.8330 - val_loss: 0.5905 - val_acc: 0.7964\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 10s - loss: 0.4014 - acc: 0.8380 - val_loss: 0.4691 - val_acc: 0.7697\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 11s - loss: 0.4010 - acc: 0.8330 - val_loss: 0.6453 - val_acc: 0.7774\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 11s - loss: 0.3919 - acc: 0.8350 - val_loss: 0.5430 - val_acc: 0.8003\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 11s - loss: 0.3965 - acc: 0.8450 - val_loss: 0.4866 - val_acc: 0.7774\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 10s - loss: 0.4227 - acc: 0.8230 - val_loss: 0.4924 - val_acc: 0.7863\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 11s - loss: 0.4074 - acc: 0.8330 - val_loss: 0.4851 - val_acc: 0.7545\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 11s - loss: 0.3925 - acc: 0.8295 - val_loss: 0.5891 - val_acc: 0.7735\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 10s - loss: 0.4027 - acc: 0.8295 - val_loss: 0.4599 - val_acc: 0.8041\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 11s - loss: 0.3871 - acc: 0.8320 - val_loss: 0.5201 - val_acc: 0.7926\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 10s - loss: 0.4100 - acc: 0.8320 - val_loss: 0.5542 - val_acc: 0.7634\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 11s - loss: 0.4168 - acc: 0.8295 - val_loss: 0.4874 - val_acc: 0.7786\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 11s - loss: 0.3931 - acc: 0.8415 - val_loss: 0.5031 - val_acc: 0.8028\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 10s - loss: 0.3991 - acc: 0.8375 - val_loss: 0.5111 - val_acc: 0.7850\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 11s - loss: 0.4198 - acc: 0.8385 - val_loss: 0.5285 - val_acc: 0.7621\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size\n",
    ")\n",
    "\n",
    "model.save_weights(\"./first_try.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gets us to a validation accuracy of 0.79-0.81 after 50 epochs(a number that was picked arbitrarily)  \n",
    "because the model is small and uses aggressive dropout, it does not seem to be overfitting too much by that point. So at the time the Kaggle competition was launched, we would be already be \"state of art\" -- with 8% of the data, and no effort to optimize our architecture or hyperparameters. In fact, the Kaggle competition, this model would have scored in the top 100(out of 215 entrants)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the variance of the validation accuracy is fairly high, both because accuracy is a high-variance metric and because we only use 800 validation samples. A good validation strategy in such cases would be to do k-fold cross validation, but this would require training k models for every evaluation round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the bottleneck features of a pre-trained netword:90% accuracy in a minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more refined approach would be to leverage a network pre-trained on a large dataset.  \n",
    "Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset -- a model previously featured on this blog.Because the ImageNet dataset contains several \"cat\" classes(persian cat, siamese cat ...) and many \"dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem.  \n",
    "\n",
    "In Fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs vs cats classification problem extremely well. However, the method we present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"VGG16 Architecture\"](./vgg16_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy will be as follow : we will only instantiate the convolutional part of the model, everything up the the fully-connected layers. We will then run this model on our training and validation data once, recording the output(the \"bottleneck features\" from the VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we train a small fully-connected model on top of the stored features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you are woking on CPU, and we want to only do it once. Note that this prevents us from using data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import  applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = \"bottleneck_fc_model.h5\"\n",
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/validation\"\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    \n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', \"rb\"))\n",
    "    train_labels = np.array(\n",
    "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', \"rb\"))\n",
    "    validation_labels = np.array(\n",
    "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.6535 - acc: 0.7625 - val_loss: 0.2895 - val_acc: 0.8800\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.3327 - acc: 0.8550 - val_loss: 0.2939 - val_acc: 0.8788\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.2896 - acc: 0.8800 - val_loss: 0.4034 - val_acc: 0.8363\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.2463 - acc: 0.9025 - val_loss: 0.4163 - val_acc: 0.8638\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.2149 - acc: 0.9145 - val_loss: 0.2866 - val_acc: 0.9050\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.2014 - acc: 0.9260 - val_loss: 0.2901 - val_acc: 0.8925\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.1710 - acc: 0.9320 - val_loss: 0.3100 - val_acc: 0.9012\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.1574 - acc: 0.9395 - val_loss: 0.3347 - val_acc: 0.8950\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.1268 - acc: 0.9510 - val_loss: 0.3939 - val_acc: 0.8912\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.1171 - acc: 0.9530 - val_loss: 0.3534 - val_acc: 0.9025\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.1040 - acc: 0.9615 - val_loss: 0.4194 - val_acc: 0.8862\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0984 - acc: 0.9615 - val_loss: 0.4007 - val_acc: 0.9050\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0886 - acc: 0.9650 - val_loss: 0.6331 - val_acc: 0.8575\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0735 - acc: 0.9735 - val_loss: 0.5009 - val_acc: 0.8925\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0799 - acc: 0.9730 - val_loss: 0.4857 - val_acc: 0.8938\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0595 - acc: 0.9780 - val_loss: 0.5044 - val_acc: 0.8888\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0461 - acc: 0.9865 - val_loss: 0.5652 - val_acc: 0.8962\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0575 - acc: 0.9780 - val_loss: 0.7090 - val_acc: 0.8750\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0430 - acc: 0.9835 - val_loss: 0.5945 - val_acc: 0.8950\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0485 - acc: 0.9850 - val_loss: 0.6294 - val_acc: 0.8962\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0319 - acc: 0.9900 - val_loss: 0.6224 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0357 - acc: 0.9845 - val_loss: 0.5861 - val_acc: 0.9050\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0316 - acc: 0.9910 - val_loss: 0.7745 - val_acc: 0.8688\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0300 - acc: 0.9905 - val_loss: 0.6797 - val_acc: 0.9038\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0235 - acc: 0.9920 - val_loss: 0.6316 - val_acc: 0.9012\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0202 - acc: 0.9920 - val_loss: 0.8036 - val_acc: 0.8950\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0198 - acc: 0.9930 - val_loss: 0.7975 - val_acc: 0.8938\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0250 - acc: 0.9915 - val_loss: 0.8179 - val_acc: 0.8838\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0191 - acc: 0.9930 - val_loss: 0.8520 - val_acc: 0.8950\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0264 - acc: 0.9920 - val_loss: 0.8737 - val_acc: 0.8862\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0188 - acc: 0.9925 - val_loss: 0.8630 - val_acc: 0.8912\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0128 - acc: 0.9945 - val_loss: 0.8913 - val_acc: 0.8900\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0192 - acc: 0.9950 - val_loss: 0.8216 - val_acc: 0.9025\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0218 - acc: 0.9935 - val_loss: 0.8287 - val_acc: 0.8988\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0265 - acc: 0.9920 - val_loss: 0.8205 - val_acc: 0.9062\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0275 - acc: 0.9945 - val_loss: 0.8405 - val_acc: 0.8938\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0183 - acc: 0.9965 - val_loss: 0.8429 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0146 - acc: 0.9950 - val_loss: 0.9549 - val_acc: 0.8938\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0190 - acc: 0.9940 - val_loss: 0.8053 - val_acc: 0.9038\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0096 - acc: 0.9965 - val_loss: 0.8295 - val_acc: 0.9025\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0144 - acc: 0.9945 - val_loss: 1.0138 - val_acc: 0.8800\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0149 - acc: 0.9970 - val_loss: 0.9703 - val_acc: 0.8962\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0075 - acc: 0.9970 - val_loss: 1.0157 - val_acc: 0.8925\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0149 - acc: 0.9950 - val_loss: 0.8932 - val_acc: 0.8988\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0096 - acc: 0.9970 - val_loss: 1.0055 - val_acc: 0.8912\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0125 - acc: 0.9980 - val_loss: 0.9027 - val_acc: 0.8962\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0279 - acc: 0.9915 - val_loss: 0.9845 - val_acc: 0.8950\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0114 - acc: 0.9945 - val_loss: 0.8810 - val_acc: 0.8975\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0114 - acc: 0.9975 - val_loss: 0.9610 - val_acc: 0.9012\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 0s - loss: 0.0035 - acc: 0.9985 - val_loss: 0.8964 - val_acc: 0.9025\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We reach a validation accuracy of 0.90-0.91: not bat at all.  \n",
    "This is definitely partly due to the fact that the base model was trained on a dataset that already featured dogs and cats(among hundreds of other classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the top layers of a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve our previous result, we can try to to \"fine-tune\" the last convolutional block of the VGG16 model anlongside the top-level classfier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "- instantiate the convolutinoal base of VGG16 and load its weights\n",
    "- add our previously defined fully-connected model on top, and load its weights\n",
    "- freeze the layers of the VGG16 model up to the last convolutional block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./vgg16_modified.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that :  \n",
    "\n",
    "- in order to perform fine-tuning, all layers should start with properly trained weights:for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.\n",
    "\n",
    "- we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).\n",
    "\n",
    "- fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the model weights files\n",
    "weights_path = \"./vgg16_weights.h5\"\n",
    "top_model_weights_path = \"./bottleneck_fc_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"./data/train/\"\n",
    "validation_data_dir = \"./data/validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG network\n",
    "base_model = applications.VGG16(weights=\"imagenet\",  include_top=False, input_shape=(150, 150, 3))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note that it is necessary to start with a fully-trained classifier\n",
    "# including the top classifier\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model on top of the convolutional base\n",
    "# model.add(top_model)\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer = optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "    metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/anaconda3/envs/env/lib/python3.4/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=125, epochs=50, validation_steps=800)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 40s - loss: 0.6516 - acc: 0.9135 - val_loss: 0.8710 - val_acc: 0.9044\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 39s - loss: 0.6253 - acc: 0.9135 - val_loss: 0.8779 - val_acc: 0.9037\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 39s - loss: 0.4917 - acc: 0.9340 - val_loss: 0.8712 - val_acc: 0.9040\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 39s - loss: 0.6017 - acc: 0.9165 - val_loss: 0.8738 - val_acc: 0.9042\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 39s - loss: 0.5784 - acc: 0.9175 - val_loss: 0.8799 - val_acc: 0.9035\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 39s - loss: 0.5852 - acc: 0.9085 - val_loss: 0.8735 - val_acc: 0.9043\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 39s - loss: 0.6901 - acc: 0.9105 - val_loss: 0.8715 - val_acc: 0.9042\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 39s - loss: 0.6719 - acc: 0.9165 - val_loss: 0.8759 - val_acc: 0.9037\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 39s - loss: 0.6538 - acc: 0.9115 - val_loss: 0.8745 - val_acc: 0.9040\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 39s - loss: 0.7513 - acc: 0.9005 - val_loss: 0.8697 - val_acc: 0.9044\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 39s - loss: 0.5701 - acc: 0.9275 - val_loss: 0.8787 - val_acc: 0.9037\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 39s - loss: 0.6203 - acc: 0.9155 - val_loss: 0.8745 - val_acc: 0.9041\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 39s - loss: 0.6324 - acc: 0.9120 - val_loss: 0.8711 - val_acc: 0.9039\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 39s - loss: 0.6114 - acc: 0.9120 - val_loss: 0.8775 - val_acc: 0.9036\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 39s - loss: 0.6038 - acc: 0.9195 - val_loss: 0.8708 - val_acc: 0.9041\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 39s - loss: 0.5396 - acc: 0.9230 - val_loss: 0.8737 - val_acc: 0.9039\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 39s - loss: 0.5760 - acc: 0.9230 - val_loss: 0.8800 - val_acc: 0.9039\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 39s - loss: 0.6117 - acc: 0.9100 - val_loss: 0.8680 - val_acc: 0.9043\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 39s - loss: 0.6498 - acc: 0.9120 - val_loss: 0.8683 - val_acc: 0.9045\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 39s - loss: 0.6486 - acc: 0.9115 - val_loss: 0.8833 - val_acc: 0.9028\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 39s - loss: 0.6402 - acc: 0.9125 - val_loss: 0.8706 - val_acc: 0.9047\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 39s - loss: 0.6819 - acc: 0.9125 - val_loss: 0.8721 - val_acc: 0.9040\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 39s - loss: 0.6274 - acc: 0.9155 - val_loss: 0.8752 - val_acc: 0.9042\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 39s - loss: 0.6340 - acc: 0.9145 - val_loss: 0.8770 - val_acc: 0.9038\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 38s - loss: 0.7035 - acc: 0.9100 - val_loss: 0.8765 - val_acc: 0.9041\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 38s - loss: 0.7074 - acc: 0.9120 - val_loss: 0.8720 - val_acc: 0.9038\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 39s - loss: 0.5651 - acc: 0.9185 - val_loss: 0.8750 - val_acc: 0.9039\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 39s - loss: 0.6480 - acc: 0.9080 - val_loss: 0.8730 - val_acc: 0.9043\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 39s - loss: 0.6000 - acc: 0.9195 - val_loss: 0.8722 - val_acc: 0.9042\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 39s - loss: 0.6488 - acc: 0.9155 - val_loss: 0.8804 - val_acc: 0.9031\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 38s - loss: 0.5872 - acc: 0.9180 - val_loss: 0.8718 - val_acc: 0.9043\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 39s - loss: 0.5515 - acc: 0.9200 - val_loss: 0.8725 - val_acc: 0.9044\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 39s - loss: 0.6099 - acc: 0.9135 - val_loss: 0.8764 - val_acc: 0.9040\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 39s - loss: 0.6288 - acc: 0.9150 - val_loss: 0.8617 - val_acc: 0.9049\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 39s - loss: 0.6904 - acc: 0.9090 - val_loss: 0.8857 - val_acc: 0.9029\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 39s - loss: 0.5857 - acc: 0.9200 - val_loss: 0.8703 - val_acc: 0.9045\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 39s - loss: 0.6851 - acc: 0.9135 - val_loss: 0.8736 - val_acc: 0.9039\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 39s - loss: 0.6266 - acc: 0.9060 - val_loss: 0.8722 - val_acc: 0.9043\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 38s - loss: 0.6954 - acc: 0.9100 - val_loss: 0.8786 - val_acc: 0.9035\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 39s - loss: 0.6692 - acc: 0.9085 - val_loss: 0.8727 - val_acc: 0.9041\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 39s - loss: 0.6297 - acc: 0.9115 - val_loss: 0.8782 - val_acc: 0.9038\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 39s - loss: 0.6439 - acc: 0.9100 - val_loss: 0.8745 - val_acc: 0.9039\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 39s - loss: 0.7091 - acc: 0.9105 - val_loss: 0.8747 - val_acc: 0.9038\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 39s - loss: 0.6061 - acc: 0.9145 - val_loss: 0.8758 - val_acc: 0.9035\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 38s - loss: 0.6427 - acc: 0.9165 - val_loss: 0.8743 - val_acc: 0.9043\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 39s - loss: 0.5847 - acc: 0.9195 - val_loss: 0.8705 - val_acc: 0.9043\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 39s - loss: 0.5890 - acc: 0.9150 - val_loss: 0.8698 - val_acc: 0.9043\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 39s - loss: 0.5925 - acc: 0.9150 - val_loss: 0.8680 - val_acc: 0.9046\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 39s - loss: 0.6064 - acc: 0.9180 - val_loss: 0.8752 - val_acc: 0.9041\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 39s - loss: 0.6590 - acc: 0.9135 - val_loss: 0.8726 - val_acc: 0.9038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85edc35320>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"./fine_tuning_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda_env",
   "language": "python",
   "name": "anaconda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
